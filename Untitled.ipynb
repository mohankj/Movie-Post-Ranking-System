{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd37302c-1da9-42f5-b3ad-59ef38c3d8c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: emoji in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (2.14.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohan244643\\appdata\\local\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Mohan244643\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Mohan244643\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mohan244643\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Mohan244643\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Mohan244643\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Mohan244643\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob emoji pandas\n",
    "!python -m textblob.download_corpora  # For sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e354263f-ce71-493d-87d3-d3fe2de433e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohan244643\\AppData\\Local\\anaconda3\\python.exe\n",
      "C:\\Users\\Mohan244643\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n",
      "C:\\Users\\Mohan244643\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n",
      "C:\\Users\\Mohan244643\\AppData\\Local\\anaconda3\\Scripts\\pip.exe\n",
      "C:\\Users\\Mohan244643\\AppData\\Local\\Programs\\Python\\Python313\\Scripts\\pip.exe\n"
     ]
    }
   ],
   "source": [
    "!where python\n",
    "!where pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0ed205-9157-4ecf-9bb3-ba29675fca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import emoji\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def calculate_post_score(post):\n",
    "    \"\"\"\n",
    "    Calculate a score (0-100) for ranking posts with enhanced Hinglish and emoji support.\n",
    "    Input: Post = {\n",
    "        \"text\": str,\n",
    "        \"likes\": int,\n",
    "        \"comments\": int,\n",
    "        \"author_content_watched\": int,\n",
    "        \"author_reviews_posted\": int,\n",
    "        \"author_public_watchlists\": int,\n",
    "        \"media_count\": int,\n",
    "        \"created_at\": datetime,\n",
    "    }\n",
    "    \"\"\"\n",
    "    # (1) Engagement Score (50% weight)\n",
    "    engagement = (\n",
    "        0.6 * min(post[\"likes\"] / 100, 1.0) + \n",
    "        0.4 * min(post[\"comments\"] / 50, 1.0)\n",
    "    )\n",
    "\n",
    "    # (2) Content Quality (30% weight)\n",
    "    ## Enhanced Sentiment Analysis (TextBlob + Emoji)\n",
    "    sentiment = TextBlob(post[\"text\"]).sentiment.polarity  # -1 to 1\n",
    "    emoji_score = calculate_emoji_sentiment(post[\"text\"])\n",
    "    combined_sentiment = (sentiment + emoji_score + 1) / 2  # Convert to 0-1 scale\n",
    "\n",
    "    ## Enhanced Hinglish Movie Relevance\n",
    "    movie_mentioned_score = check_movie_relevance(post[\"text\"])\n",
    "\n",
    "    ## Media Boost (Images/Videos)\n",
    "    media_boost = min(post[\"media_count\"] / 4, 1.0)\n",
    "\n",
    "    content = (\n",
    "        0.5 * combined_sentiment +\n",
    "        0.3 * movie_mentioned_score +\n",
    "        0.2 * media_boost\n",
    "    )\n",
    "\n",
    "    # (3) Author Reputation (10% weight) - unchanged\n",
    "    author = (\n",
    "        0.4 * min(post[\"author_content_watched\"] / 10000, 1.0) +  \n",
    "        0.3 * min(post[\"author_reviews_posted\"] / 1000, 1.0) +\n",
    "        0.3 * min(post[\"author_public_watchlists\"] / 1000, 1.0)\n",
    "    )\n",
    "\n",
    "    # (4) Time Decay (10% weight) - unchanged\n",
    "    hours_old = (datetime.now() - post[\"created_at\"]).total_seconds() / 3600\n",
    "    decay = 0.5 ** (hours_old / 48)  # Halflife = 2 days\n",
    "\n",
    "    # Final Weighted Score (0-100)\n",
    "    raw_score = (0.5 * engagement + 0.3 * content + 0.1 * author) * 100\n",
    "    final_score = decay * raw_score\n",
    "\n",
    "    return round(final_score, 1)\n",
    "\n",
    "def calculate_emoji_sentiment(text):\n",
    "    \"\"\"Calculate sentiment from emojis (returns value between -0.5 to +0.5)\"\"\"\n",
    "    emoji_sentiment_map = {\n",
    "        # Positive emojis\n",
    "        '‚ù§Ô∏è': 0.3, 'üëå': 0.3, 'üëç': 0.2, 'üòç': 0.3, 'üî•': 0.2,\n",
    "        'üéâ': 0.2, 'ü§©': 0.3, 'üôå': 0.2, 'üíØ': 0.3, 'üòä': 0.2,\n",
    "        # Negative emojis\n",
    "        'üëé': -0.3, 'üòí': -0.2, 'üíî': -0.3, 'üò†': -0.3, 'ü§Æ': -0.4,\n",
    "        'üò§': -0.2, 'üòë': -0.1, 'üôÑ': -0.2\n",
    "    }\n",
    "    \n",
    "    total_score = 0\n",
    "    emojis = [c for c in text if c in emoji.EMOJI_DATA]\n",
    "    \n",
    "    for e in emojis:\n",
    "        total_score += emoji_sentiment_map.get(e, 0)\n",
    "    \n",
    "    # Normalize to prevent overpowering\n",
    "    return max(-0.5, min(0.5, total_score))\n",
    "\n",
    "def check_movie_relevance(text):\n",
    "    \"\"\"Enhanced Hinglish movie detection (returns score 0-1)\"\"\"\n",
    "    # English movie terms\n",
    "    english_terms = [\n",
    "        \"movie\", \"film\", \"cinema\", \"bollywood\", \"hollywood\",\n",
    "        \"watch\", \"review\", \"actor\", \"actress\", \"director\",\n",
    "        \"scene\", \"ending\", \"plot\", \"story\", \"screenplay\"\n",
    "    ]\n",
    "    \n",
    "    # Common Hinglish phrases about movies\n",
    "    hinglish_phrases = [\n",
    "        \"paisa vasool\", \"time pass\", \"mind blowing\",\n",
    "        \"must watch\", \"mat dekho\", \"bakwas movie\",\n",
    "        \"hit hai\", \"flop hai\", \"time waste\"\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Check for exact matches\n",
    "    term_matches = (\n",
    "        sum(1 for term in english_terms if term in text_lower) +\n",
    "        sum(1 for term in hinglish_terms if term in text_lower) +\n",
    "        sum(1 for phrase in hinglish_phrases if phrase in text_lower)\n",
    "    )\n",
    "    \n",
    "    # Return score based on number of matches (0.2 base score if no matches)\n",
    "    return min(1.0, 0.2 + (term_matches * 0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5e83fb7-75be-4b2d-abfb-6b947bc9c5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 25.4 | Text: Superman Discussion thread. Tell us how you enjoyed the movie‚úåÔ∏è\n",
      "Score: 13.9 | Text: CBFC ne Superman movie se kissiya kaat di bhai, SUPERMAN MOVIE SE! I'm equally baffled and disappointed at the same time.\n"
     ]
    }
   ],
   "source": [
    "posts = [\n",
    "    {\n",
    "        \"text\": \"Superman Discussion thread. Tell us how you enjoyed the movie‚úåÔ∏è\",\n",
    "        \"likes\": 22,\n",
    "        \"comments\": 6,\n",
    "        \"author_content_watched\": 153,\n",
    "        \"author_reviews_posted\": 135,\n",
    "        \"author_public_watchlists\": 5,\n",
    "        \"media_count\": 1,\n",
    "        \"created_at\": datetime.now() - timedelta(hours=0)\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"CBFC ne Superman movie se kissiya kaat di bhai, SUPERMAN MOVIE SE! I'm equally baffled and disappointed at the same time.\",\n",
    "        \"likes\": 18,\n",
    "        \"comments\": 7,\n",
    "        \"author_content_watched\": 195,\n",
    "        \"author_reviews_posted\": 188,\n",
    "        \"author_public_watchlists\": 2,\n",
    "        \"media_count\": 1,\n",
    "        \"created_at\": datetime.now() - timedelta(hours=22)\n",
    "    }\n",
    "]\n",
    "\n",
    "for post in posts:\n",
    "    print(f\"Score: {calculate_post_score(post)} | Text: {post['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e568ca4-df57-4749-86be-6d0aeb4b83e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00304629b2dc4cb3845bd1f35e4b35f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohan244643\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Mohan244643\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf9b9dac54247f3b9604f128a5289cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d3b7e03db44e72b08ec996f771ca18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f9192b150e4628b4c9b95146067784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef095ca1862246cf819bea8074e10291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize once at app startup\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "text_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948122d4-b5cc-453a-ad8f-9113719c95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_content(text):\n",
    "    # Sentiment analysis\n",
    "    sentiment_result = sentiment_analyzer(text[:512])  # Truncate to model limit\n",
    "    sentiment_score = 1 if sentiment_result[0]['label'] == 'POSITIVE' else 0\n",
    "    \n",
    "    # Movie relevance classification\n",
    "    candidate_labels = [\"movie review\", \"film discussion\", \"off-topic\"]\n",
    "    classification = text_classifier(text[:512], candidate_labels)\n",
    "    movie_score = classification['scores'][candidate_labels.index(\"movie review\")] \n",
    "    \n",
    "    return {\n",
    "        \"sentiment\": sentiment_score,\n",
    "        \"movie_relevance\": movie_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ab64fd0-fcf4-40c0-ae98-e832e8f2f94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 0, 'movie_relevance': 0.5982581973075867}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_content(\"CBFC ne Superman movie se kissiya kaat di bhai, SUPERMAN MOVIE SE! I'm equally baffled and disappointed at the same time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f458ee-65a0-4579-a341-37a2c9c2ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class PostAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Initialize models once (at app startup)\n",
    "        self.sentiment_analyzer = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "        )\n",
    "        self.relevance_classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=\"joeddav/xlm-roberta-large-xnli\"\n",
    "        )\n",
    "    \n",
    "    def calculate_post_score(self, post):\n",
    "        \"\"\"Enhanced scoring with pre-trained models\"\"\"\n",
    "        # (1) Engagement Score (unchanged)\n",
    "        engagement = (\n",
    "            0.6 * min(post[\"likes\"] / 100, 1.0) + \n",
    "            0.4 * min(post[\"comments\"] / 50, 1.0)\n",
    "        )\n",
    "\n",
    "        # (2) Content Quality - Now using ML models\n",
    "        analysis = self._analyze_content(post[\"text\"])\n",
    "        \n",
    "        content = (\n",
    "            0.5 * analysis[\"sentiment\"] +\n",
    "            0.3 * analysis[\"movie_relevance\"] +\n",
    "            0.2 * min(post[\"media_count\"] / 4, 1.0)\n",
    "        )\n",
    "\n",
    "        # (3) Author Reputation (unchanged)\n",
    "        author = (\n",
    "            0.4 * min(post[\"author_content_watched\"] / 10000, 1.0) +  \n",
    "            0.3 * min(post[\"author_reviews_posted\"] / 1000, 1.0) +\n",
    "            0.3 * min(post[\"author_public_watchlists\"] / 1000, 1.0)\n",
    "        )\n",
    "\n",
    "        # (4) Time Decay (unchanged)\n",
    "        hours_old = (datetime.now() - post[\"created_at\"]).total_seconds() / 3600\n",
    "        decay = 0.5 ** (hours_old / 48)\n",
    "\n",
    "        # Final Score\n",
    "        raw_score = (0.5 * engagement + 0.3 * content + 0.1 * author) * 100\n",
    "        return round(decay * raw_score, 1)\n",
    "\n",
    "    def _analyze_content(self, text):\n",
    "        \"\"\"Handle text analysis with pre-trained models\"\"\"\n",
    "        # Sentiment Analysis\n",
    "        try:\n",
    "            sentiment_result = self.sentiment_analyzer(text[:512])  # Trim to model limit\n",
    "            label = sentiment_result[0][\"label\"].lower()\n",
    "            score = sentiment_result[0][\"score\"]\n",
    "            \n",
    "            # Note: This model uses LABEL_0 (negative), LABEL_1 (neutral), LABEL_2 (positive)\n",
    "            if \"label_2\" in label:  # Positive\n",
    "                sentiment_score = score\n",
    "            elif \"label_0\" in label:  # Negative\n",
    "                sentiment_score = 1 - score\n",
    "            else:  # Neutral\n",
    "                sentiment_score = 0.5\n",
    "        except Exception as e:\n",
    "            print(f\"Sentiment analysis failed: {e}\")\n",
    "            sentiment_score = 0.5  # Fallback value\n",
    "\n",
    "        # Movie Relevance\n",
    "        candidate_labels = [\n",
    "            \"movie review\", \n",
    "            \"film discussion\", \n",
    "            \"off-topic\"\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            relevance_result = self.relevance_classifier(\n",
    "                text[:512], \n",
    "                candidate_labels,\n",
    "                multi_label=True\n",
    "            )\n",
    "            movie_score = max(\n",
    "                relevance_result[\"scores\"][0],  # movie review\n",
    "                relevance_result[\"scores\"][1]   # film discussion\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Relevance classification failed: {e}\")\n",
    "            movie_score = 0.2  # Fallback value\n",
    "\n",
    "        return {\n",
    "            \"sentiment\": sentiment_score,\n",
    "            \"movie_relevance\": movie_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37300db4-5f14-40d7-aa81-ba7f3c7a855b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d8befb56e842d0a65e270183d807bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohan244643\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Mohan244643\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-xlm-roberta-base-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfcba06156e495794d721abb1d948f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d85634e29840048d40f2693efedf3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389f2534132e4d28aeb98f5c3b5112f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4623b329564299a2d2ec92af583a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1594\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[1;34m(self, tiktoken_url)\u001b[0m\n\u001b[0;32m   1593\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1594\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_tiktoken_bpe\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1737\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[1;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[0;32m   1735\u001b[0m         vocab_file\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file,\n\u001b[0;32m   1736\u001b[0m         additional_special_tokens\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39madditional_special_tokens,\n\u001b[1;32m-> 1737\u001b[0m     )\u001b[38;5;241m.\u001b[39mconverted()\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1631\u001b[0m, in \u001b[0;36mTikTokenConverter.converted\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[1;32m-> 1631\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer()\n\u001b[0;32m   1632\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mSequence(\n\u001b[0;32m   1633\u001b[0m         [\n\u001b[0;32m   1634\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mSplit(Regex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern), behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misolated\u001b[39m\u001b[38;5;124m\"\u001b[39m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1635\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space, use_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1636\u001b[0m         ]\n\u001b[0;32m   1637\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1624\u001b[0m, in \u001b[0;36mTikTokenConverter.tokenizer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1624\u001b[0m     vocab_scores, merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_vocab_merges_from_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file)\n\u001b[0;32m   1625\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(BPE(vocab_scores, merges, fuse_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1596\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[1;34m(self, tiktoken_url)\u001b[0m\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m-> 1596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1597\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1598\u001b[0m     )\n\u001b[0;32m   1600\u001b[0m bpe_ranks \u001b[38;5;241m=\u001b[39m load_tiktoken_bpe(tiktoken_url)\n",
      "\u001b[1;31mValueError\u001b[0m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Usage Example\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m PostAnalyzer()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(analyzer\u001b[38;5;241m.\u001b[39mcalculate_post_score(posts))\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36mPostAnalyzer.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Initialize models once (at app startup)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiment_analyzer \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      9\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcardiffnlp/twitter-xlm-roberta-base-sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m     )\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevance_classifier \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero-shot-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoeddav/xlm-roberta-large-xnli\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1137\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m   1134\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m   1135\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1137\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m   1138\u001b[0m             tokenizer_identifier, use_fast\u001b[38;5;241m=\u001b[39muse_fast, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs\n\u001b[0;32m   1139\u001b[0m         )\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1069\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1066\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2014\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2012\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2015\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2016\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2017\u001b[0m     init_configuration,\n\u001b[0;32m   2018\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2019\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2020\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2021\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2022\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2023\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2024\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2025\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2026\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2260\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2258\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2260\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[0;32m   2262\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2265\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\tokenization_xlm_roberta_fast.py:108\u001b[0m, in \u001b[0;36mXLMRobertaTokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     94\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m ):\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    109\u001b[0m         vocab_file,\n\u001b[0;32m    110\u001b[0m         tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    111\u001b[0m         bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    112\u001b[0m         eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    113\u001b[0m         sep_token\u001b[38;5;241m=\u001b[39msep_token,\n\u001b[0;32m    114\u001b[0m         cls_token\u001b[38;5;241m=\u001b[39mcls_token,\n\u001b[0;32m    115\u001b[0m         unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    116\u001b[0m         pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[0;32m    117\u001b[0m         mask_token\u001b[38;5;241m=\u001b[39mmask_token,\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:139\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m--> 139\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(\u001b[38;5;28mself\u001b[39m, from_tiktoken\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    140\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1739\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[1;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[0;32m   1735\u001b[0m         vocab_file\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file,\n\u001b[0;32m   1736\u001b[0m         additional_special_tokens\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39madditional_special_tokens,\n\u001b[0;32m   1737\u001b[0m     )\u001b[38;5;241m.\u001b[39mconverted()\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1740\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1741\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1742\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1743\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "# Usage Example\n",
    "analyzer = PostAnalyzer()\n",
    "\n",
    "print(analyzer.calculate_post_score(posts)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e940b58c-4e02-4286-812a-aef6be8e108c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
